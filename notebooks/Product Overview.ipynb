{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ef89e6b3",
   "metadata": {},
   "source": [
    "# Product Overview\n",
    "\n",
    "This repository aims to provide tools to the NYC Department of Health and Mental Hygiene (DOHMH) that aid them in:\n",
    "- Identifying problematic areas of NYC where they frequently receive tickets\n",
    "- Forecasting future (1-week ahead) ticket flow\n",
    "- Comparing predicted patterns with historical data\n",
    "\n",
    "DOHMH was selected for several reasons:\n",
    "\n",
    "- **Interesting problem domain:** The DOHMH dataset contains compelling issues such as rodent complaints and food safety inspections\n",
    "- **Manageable data size:** With approximately 1M records, it's large enough to be meaningful but digestible enough to work with efficiently without requiring big data infrastructure\n",
    "\n",
    "## Technical Problem Statement\n",
    "\n",
    "**Core question:** How many tickets will be opened by next week?\n",
    "\n",
    "## Technical Deliverables\n",
    "\n",
    "1. **Predictive Models** - Machine learning models to forecast ticket volume and characteristics\n",
    "2. **Interactive Dashboard** - Visualization tool to explore predictions and run historical scenario analyses\n",
    "\n",
    "## Business Value\n",
    "DOHMH agents’ schedules are often set at the last minute because their work is request-based. A tool that provides well-informed estimates of future ticket demand can make a significant difference in planning and resource allocation. While point estimates are useful for technical users, ranges are generally more impactful. Therefore, this tool also provides quantile estimates to offer greater clarity and actionable insights.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa5a6681",
   "metadata": {},
   "source": [
    "## Original Model Architecture Plan\n",
    "\n",
    "The initial vision for this project included **three types of predictive models** working in concert:\n",
    "\n",
    "### 1. Forecast Model\n",
    "Predicts the number of new requests expected on a daily basis (1-7 days ahead) by location and complaint type.\n",
    "\n",
    "### 2. Severity/Triage Model\n",
    "Assigns priority scores to tickets based on the probability of key risk factors:\n",
    "- **Inspection requirement:** Whether the resolution requires an inspection\n",
    "- **SLA breach:** Whether the ticket will exceed its service level agreement due date\n",
    "\n",
    "### 3. Duration Model\n",
    "Estimates how long it will take for a ticket to be closed once it's picked up (with censoring for tickets still in progress).\n",
    "\n",
    "### Prioritization Strategy\n",
    "\n",
    "The three models would work together to create a comprehensive resource allocation strategy:\n",
    "\n",
    "1. **Risk-adjusted hours:** Combine severity probability with duration estimates: `Severity × Duration = Risk-adjusted hours`\n",
    "\n",
    "2. **Spatial aggregation:** Summarize tickets at the H3 hexagon level to calculate total severity-weighted hours for each geographic cell\n",
    "\n",
    "3. **Dynamic prioritization:** Take the number of available inspectors as input and show which H3 cells should be prioritized based on sorted severity-weighted workload\n",
    "\n",
    "### Implementation Status\n",
    "\n",
    "Due to time constraints and the complexity of these problems (particularly the severity/triage and duration models), **the current implementation focuses on the forecasting model**. The severity and duration models, while architecturally designed, remain future work.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a75a5e08",
   "metadata": {},
   "source": [
    "## Environment Setup\n",
    "\n",
    "The NYC Open Data API (Socrata) requires authentication credentials. This notebook assumes that `SOCRATA_APP_TOKEN`, `SOCRATA_API_KEY_ID`, and `SOCRATA_API_KEY_SECRET` are set in a `.env` file in the project root directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4da8cb9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f3812f3",
   "metadata": {},
   "source": [
    "### Load Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f22fc662",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\gorav\\GitHub\\nyc-311-service-requests\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "PACKAGE_PATH = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "sys.path.insert(0, PACKAGE_PATH)\n",
    "\n",
    "from src import train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dbedacc",
   "metadata": {},
   "source": [
    "## Train\n",
    "\n",
    "The training pipeline orchestrates the end-to-end process of building forecast models for NYC 311 service requests. This includes data fetching, preprocessing, feature engineering, and training multiple quantile regression models to provide both point estimates and uncertainty bounds.\n",
    "\n",
    "### Pipeline Overview\n",
    "\n",
    "The `train_models()` function executes the following steps:\n",
    "\n",
    "1. **Data Fetching (Optional):**\n",
    "   - NYC 311 Service Requests from Socrata Open Data API (DOHMH agency only)\n",
    "   - ACS Census Population data (2013-2023) via `censusdata` package\n",
    "   - NOAA Weather data (2010-2025) via NCEI/NOAA Climate Data API\n",
    "\n",
    "2. **Data Preprocessing:**\n",
    "   - Merges 311 requests with external data sources (weather, population)\n",
    "   - Cleans and standardizes data types\n",
    "   - Filters relevant date ranges and removes invalid records\n",
    "\n",
    "3. **Feature Engineering:**\n",
    "   - Creates temporal features: lags (1, 4 weeks), rolling averages (4, 12 weeks), momentum\n",
    "   - Adds weather features: temperature, precipitation, heating/cooling degree days, 3-day/7-day rain\n",
    "   - Includes spatial features: neighborhood rolling averages, log population\n",
    "   - Builds categorical features: month, heat/freeze flags, COVID era flag\n",
    "   - Constructs weekly forecast panel at the (week, H3 hex, complaint family) grain\n",
    "\n",
    "4. **Model Training:**\n",
    "   - Trains 4 separate LightGBM models for different quantile forecasts:\n",
    "     - **Mean model:** Point estimate of expected requests\n",
    "     - **50th percentile:** Median forecast\n",
    "     - **10th percentile:** Lower uncertainty bound\n",
    "     - **90th percentile:** Upper uncertainty bound\n",
    "   - Each model predicts 1-week ahead service request counts\n",
    "   - Uses optimal hyperparameters from prior tuning (stored in `model_optimal_params.json`)\n",
    "\n",
    "5. **Model Persistence:**\n",
    "   - Saves trained model bundles with timestamp (format: `YYYYMMDD_HHMMSS`)\n",
    "   - Each bundle includes preprocessor and trained model\n",
    "   - Stored in `models/{timestamp}/full_bundle/`\n",
    "\n",
    "### Parameters\n",
    "\n",
    "- **`run_fetch`** (bool, default=False): \n",
    "  - If `True`, fetches fresh data from external APIs (311, ACS, NOAA)\n",
    "  - If `False`, uses existing local data files\n",
    "  - Note: API credentials required via environment variables\n",
    "\n",
    "- **`save_data`** (bool, default=False): \n",
    "  - If `True`, saves preprocessed data and feature-engineered datasets to S3 bucket\n",
    "  - If `False`, only processes data in-memory\n",
    "  - Note: S3 bucket (`s3://hbc-technical-assessment-gk/`) requires AWS credentials with write access\n",
    "\n",
    "- **`save_models`** (bool, default=False): \n",
    "  - If `True`, persists trained model bundles to local disk\n",
    "  - If `False`, models exist only in-memory during the session\n",
    "  - Recommended: `True` for production runs\n",
    "\n",
    "### Recommended Settings\n",
    "\n",
    "For typical retraining runs (using existing data, saving new models):\n",
    "\n",
    "```python\n",
    "train.train_models(run_fetch=False, save_data=False, save_models=True, save_models_s3=False)\n",
    "```\n",
    "\n",
    "**Rationale:**\n",
    "- **`run_fetch=False`**: Data is frozen in s3 and should only be set to True when fresh data is needed\n",
    "- **`save_data=False`**: S3 bucket write access is restricted\n",
    "- **`save_models=True`**: Persisting models enables deployment to Streamlit app and future inference\n",
    "- **`save_models=False`**: S3 bucket write access is restricted\n",
    "\n",
    "\n",
    "### Continuous Retraining Strategy\n",
    "\n",
    "This pipeline is designed to support **automated continuous retraining** for production use:\n",
    "\n",
    "#### Scheduled Retraining Workflow\n",
    "\n",
    "1. **Weekly/Monthly Schedule:**\n",
    "   - Set up a cron job or orchestration tool (Airflow, Prefect, GitHub Actions) to run the pipeline periodically\n",
    "   - Recommended cadence: Weekly (Monday mornings) to incorporate the latest week's closed tickets\n",
    "\n",
    "2. **Full Pipeline Run:**\n",
    "   ```python\n",
    "   # Fetch latest data, retrain models, save everything\n",
    "   train.train_models(run_fetch=True, save_data=True, save_models=True)\n",
    "   ```\n",
    "\n",
    "3. **Model Versioning:**\n",
    "   - Each training run creates a timestamped model directory\n",
    "   - Update `config.MODEL_TIMESTAMP` to point to the latest version\n",
    "\n",
    "4. **Validation & Deployment:**\n",
    "   - Run automated tests on new models (accuracy thresholds, prediction ranges)\n",
    "   - Compare performance metrics against previous version\n",
    "\n",
    "This approach ensures models stay current with evolving patterns (seasonality, COVID transitions, demographic changes) without manual intervention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "41523da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train.train_models(run_fetch = False, save_data = False, save_models = True, save_models_s3 = False) # takes ~8-15 minutes depending on setting for save_models_s3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f1a844e",
   "metadata": {},
   "source": [
    "## Streamlit App\n",
    "\n",
    "The Streamlit app provides an interactive dashboard for exploring model performance and forecasting results. Users can visualize predictions versus actual service request patterns across different geographic areas and complaint types.\n",
    "\n",
    "### Key Features\n",
    "\n",
    "- **Weekly Analysis:** View predictions and actual service requests for any week in the past 52 weeks\n",
    "- **Complaint Family Breakdown:** Explore 10 different complaint categories including:\n",
    "  - Food Safety\n",
    "  - Vector Control (rodents, mosquitoes, pigeons)\n",
    "  - Housing Health\n",
    "  - Animal Control\n",
    "  - Air/Smoke/Mold\n",
    "  - Hazmat/Lead/Asbestos\n",
    "  - Childcare/Recreation\n",
    "  - COVID-19 violations\n",
    "  - Water Quality\n",
    "  - Miscellaneous\n",
    "- **Multiple Prediction Views:** Compare actual values against:\n",
    "  - Mean predictions\n",
    "  - 50th percentile (median) predictions\n",
    "  - 10th and 90th percentile predictions (uncertainty bounds)\n",
    "- **Geographic Visualization:** Interactive H3 hexagon maps showing spatial distribution of service requests\n",
    "- **Performance Metrics:** Summary statistics including total requests, hexagon-level aggregations\n",
    "\n",
    "### How to Run\n",
    "\n",
    "```bash\n",
    "streamlit run streamlit_app/app.py\n",
    "```\n",
    "\n",
    "The first time you run it, it wil take 10-15 seconds to load as it read the data for the first time.\n",
    "\n",
    "### Prerequisites\n",
    "\n",
    "- Trained models must be available in the `models/` directory\n",
    "- Preprocessed data (`streamlit_data.parquet`) must be in `streamlit_app/resources/`\n",
    "- All required packages from `requirements.txt` must be installed\n",
    "\n",
    "> Note: Most recent week possible is `2025-07-29` because of lag on weather data reporting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78d1968e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# command as text for easier copy and pasting\n",
    "# streamlit run streamlit_app/app.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
